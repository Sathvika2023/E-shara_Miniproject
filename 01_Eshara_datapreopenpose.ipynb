{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathvika2023/MIniproject/blob/main/01_Eshara_datapreopenpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddYp3pemTIDm",
        "outputId": "02572dae-983b-4abb-b71f-9eb4de5de6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck2WLttjTV2n",
        "outputId": "e74d7cc7-beaf-49d0-d84e-0ec900ff24f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.11/dist-packages/tensorflow-2.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Collecting tensorflow==2.12.0\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Install required Python packages\n",
        "!pip install -q numpy==1.23.5 opencv-python tqdm mediapipe tensorflow==2.12.0 moviepy gdown\n",
        "\n",
        "# Optional: install unzip utility (only needed if you're downloading zip files)\n",
        "!apt-get -qq install -y unzip\n",
        "# Install the correct version of NumPy (if itâ€™s not already installed)\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "# Uninstall the current TensorFlow installation\n",
        "!pip uninstall tensorflow\n",
        "\n",
        "# Reinstall TensorFlow with the correct NumPy version linked\n",
        "!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDjq9VGpTl-u",
        "outputId": "2d77332b-ad84-4d82-a856-19b90ee7ee35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cell 3: Imports\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from moviepy.editor import VideoFileClip\n",
        "import mediapipe as mp\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jrOGgyNTtUU",
        "outputId": "64be5b2d-f0d7-4a4a-b825-c96cc87c6915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model to /usr/local/lib/python3.11/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(\n",
        "    static_image_mode=False,\n",
        "    model_complexity=2,\n",
        "    enable_segmentation=False,\n",
        "    min_detection_confidence=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg1fGGEdT7Pt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configuration\n",
        "DATASET_PATH = \"/content/drive/MyDrive/datauk.zip\"  # Update this\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/processed_openpose_data\"\n",
        "IMG_SIZE = (368, 368)  # OpenPose standard input size\n",
        "HEATMAP_SIZE = (46, 46)  # 1/8 of input size\n",
        "SEQ_LENGTH = 16  # For temporal processing\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hw_FZB7UGTo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "from moviepy.editor import VideoFileClip\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False,\n",
        "                   min_detection_confidence=0.5,\n",
        "                   min_tracking_confidence=0.5)\n",
        "\n",
        "def process_videos_to_openpose(video_dir, output_dir, frames_per_video=50):\n",
        "    \"\"\"Extracts frames and OpenPose keypoints from videos, handling nested zip structures\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    all_frames = []\n",
        "    all_keypoints = []\n",
        "    processed_words = set()\n",
        "\n",
        "    print(f\"ğŸ“‚ Scanning: {video_dir}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    def process_video_file(video_path, word_category=\"\"):\n",
        "        \"\"\"Helper function to process individual video files\"\"\"\n",
        "        try:\n",
        "            clip = VideoFileClip(video_path)\n",
        "            video_name = Path(video_path).stem\n",
        "\n",
        "            # Create output subdirectory for word category\n",
        "            word_output_dir = os.path.join(output_dir, word_category)\n",
        "            os.makedirs(word_output_dir, exist_ok=True)\n",
        "\n",
        "            frames = []\n",
        "            keypoints = []\n",
        "\n",
        "            for frame_idx, frame in enumerate(clip.iter_frames()):\n",
        "                if frame_idx % max(1, int(clip.fps / frames_per_video)) == 0:\n",
        "                    # Save frame\n",
        "                    frame_path = os.path.join(word_output_dir,\n",
        "                                            f\"{word_category}_{video_name}_{frame_idx}.jpg\")\n",
        "                    success = cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    if not success:\n",
        "                        print(f\"âŒ Failed to save: {frame_path}\")\n",
        "                        continue\n",
        "\n",
        "                    frames.append(frame_path)\n",
        "\n",
        "                    # Get keypoints\n",
        "                    results = pose.process(frame)\n",
        "                    if results.pose_landmarks:\n",
        "                        kps = []\n",
        "                        for landmark in results.pose_landmarks.landmark:\n",
        "                            kps.extend([landmark.x, landmark.y, landmark.visibility])\n",
        "                        keypoints.append(kps)\n",
        "                    else:\n",
        "                        keypoints.append([])\n",
        "\n",
        "            clip.close()\n",
        "            return frames, keypoints\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error processing {video_path}: {str(e)}\")\n",
        "            return [], []\n",
        "\n",
        "    if video_dir.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(video_dir, 'r') as zip_ref:\n",
        "            # Find all video files, preserving directory structure\n",
        "            video_files = [f for f in zip_ref.namelist()\n",
        "                         if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "            print(f\"ğŸ Found {len(video_files)} videos in zip\")\n",
        "\n",
        "            for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
        "                # Extract word category from path (assuming structure: words/word1/videos.mp4)\n",
        "                path_parts = Path(video_file).parts\n",
        "                word_category = path_parts[-2] if len(path_parts) > 1 else \"unknown\"\n",
        "\n",
        "                if word_category not in processed_words:\n",
        "                    print(f\"\\nProcessing word category: {word_category}\")\n",
        "                    processed_words.add(word_category)\n",
        "\n",
        "                # Extract and process video\n",
        "                with zip_ref.open(video_file) as video_data:\n",
        "                    temp_path = f\"temp_{Path(video_file).name}\"\n",
        "                    with open(temp_path, \"wb\") as temp_file:\n",
        "                        temp_file.write(video_data.read())\n",
        "\n",
        "                    frames, kps = process_video_file(temp_path, word_category)\n",
        "                    all_frames.extend(frames)\n",
        "                    all_keypoints.extend(kps)\n",
        "\n",
        "                    # Clean up\n",
        "                    if os.path.exists(temp_path):\n",
        "                        os.remove(temp_path)\n",
        "    else:\n",
        "        # Process directory with word subfolders\n",
        "        for root, dirs, files in os.walk(video_dir):\n",
        "            video_files = [f for f in files if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "            if video_files:\n",
        "                word_category = Path(root).name\n",
        "                print(f\"\\nProcessing word category: {word_category}\")\n",
        "\n",
        "                for video_file in tqdm(video_files, desc=f\"Processing {word_category}\"):\n",
        "                    video_path = os.path.join(root, video_file)\n",
        "                    frames, kps = process_video_file(video_path, word_category)\n",
        "                    all_frames.extend(frames)\n",
        "                    all_keypoints.extend(kps)\n",
        "\n",
        "    print(f\"\\nâœ… Done. Extracted {len(all_frames)} frames and {len(all_keypoints)} keypoints sets.\")\n",
        "    print(f\"Processed {len(processed_words)} word categories: {sorted(processed_words)}\")\n",
        "\n",
        "    return all_frames, all_keypoints\n",
        "\n",
        "# Example usage:\n",
        "# frames, keypoints = process_videos_to_openpose(\"dataset.zip\", \"output_frames\")\n",
        "# OR\n",
        "# frames, keypoints = process_videos_to_openpose(\"dataset_folder\", \"output_frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzB-RkvTURii"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 5: Heatmap Generator\n",
        "def generate_heatmaps(keypoints_list):\n",
        "    \"\"\"Converts keypoints to Gaussian heatmaps\"\"\"\n",
        "    heatmaps = []\n",
        "    for kps in keypoints_list:\n",
        "        heatmap = np.zeros((*HEATMAP_SIZE, len(kps)//3), dtype=np.float32)\n",
        "\n",
        "        for i in range(0, len(kps), 3):\n",
        "            x, y, v = kps[i], kps[i+1], kps[i+2]\n",
        "            if v > 0.1:  # Only visible keypoints\n",
        "                x_hm = int(x * HEATMAP_SIZE[1])\n",
        "                y_hm = int(y * HEATMAP_SIZE[0])\n",
        "\n",
        "                # Create 2D Gaussian\n",
        "                xx, yy = np.meshgrid(np.arange(HEATMAP_SIZE[1]), np.arange(HEATMAP_SIZE[0]))\n",
        "                heatmap[..., i//3] = np.exp(-((xx - x_hm)**2 + (yy - y_hm)**2) / (2 * 2.5**2))\n",
        "\n",
        "        heatmaps.append(heatmap)\n",
        "    return heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYpbwx8KUXDX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Cell 6: Data Augmentation\n",
        "def augment_data(image, heatmap):\n",
        "    \"\"\"Applies random augmentations\"\"\"\n",
        "    # Random flip\n",
        "    if np.random.rand() > 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "        heatmap = np.flip(heatmap, axis=1)\n",
        "\n",
        "    # Random brightness\n",
        "    alpha = np.random.uniform(0.8, 1.2)\n",
        "    image = cv2.convertScaleAbs(image, alpha=alpha)\n",
        "\n",
        "    return image, heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez4EUEkJXIY7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsHjuVPQUbvY"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Sequence Generator for CNN+RNN\n",
        "import tensorflow as tf # Import tensorflow before using it\n",
        "SEQ_LENGTH = 16\n",
        "class OpenPoseSequenceGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, frame_paths, keypoints, batch_size=8, seq_length=SEQ_LENGTH, augment=True):\n",
        "        self.frame_paths = frame_paths\n",
        "        self.keypoints = keypoints\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(frame_paths) - seq_length + 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_heatmaps = []\n",
        "\n",
        "        for i in batch_indices:\n",
        "            sequence_images = []\n",
        "            sequence_heatmaps = []\n",
        "\n",
        "            for j in range(i, i+self.seq_length):\n",
        "                # Load frame\n",
        "                img = cv2.imread(self.frame_paths[j])\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, IMG_SIZE).astype(np.float32)/255.0\n",
        "\n",
        "                # Generate heatmap\n",
        "                heatmap = np.zeros((*HEATMAP_SIZE, len(self.keypoints[j])//3), dtype=np.float32)\n",
        "                for k in range(0, len(self.keypoints[j]), 3):\n",
        "                    x, y, v = self.keypoints[j][k], self.keypoints[j][k+1], self.keypoints[j][k+2]\n",
        "                    if v > 0.1:\n",
        "                        x_hm = int(x * HEATMAP_SIZE[1])\n",
        "                        y_hm = int(y * HEATMAP_SIZE[0])\n",
        "                        heatmap[y_hm-1:y_hm+2, x_hm-1:x_hm+2, k//3] = 1.0\n",
        "\n",
        "                # Augment if training\n",
        "                if self.augment:\n",
        "                    img, heatmap = augment_data(img, heatmap)\n",
        "\n",
        "                sequence_images.append(img)\n",
        "                sequence_heatmaps.append(heatmap)\n",
        "\n",
        "            batch_images.append(np.stack(sequence_images))\n",
        "            batch_heatmaps.append(np.stack(sequence_heatmaps))\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_heatmaps)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwha9xdUmlR",
        "outputId": "05c16bdc-b035-4f10-a931-1a4dd6ee4aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing videos to OpenPose data...\n",
            "ğŸ“‚ Scanning: /content/drive/MyDrive/datauk.zip\n",
            "ğŸ Found 730 videos in zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing videos:   0%|          | 0/730 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 1. loud\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5177.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:   2%|â–         | 15/730 [07:14<4:48:07, 24.18s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9448.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:   2%|â–         | 18/730 [08:26<4:44:54, 24.01s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9534.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:   3%|â–         | 21/730 [09:52<5:18:20, 26.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 2. quiet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:   4%|â–         | 32/730 [15:00<4:48:07, 24.77s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9294.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:   6%|â–Œ         | 42/730 [19:31<5:37:12, 29.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 3. happy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:   6%|â–‹         | 46/730 [21:41<5:54:24, 31.09s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5264.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:   9%|â–Š         | 63/730 [28:46<5:02:26, 27.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 36. light\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:   9%|â–‰         | 64/730 [29:19<5:19:17, 28.77s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9712.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  10%|â–‰         | 71/730 [32:53<5:33:17, 30.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 39. famous\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  11%|â–ˆ         | 79/730 [36:22<4:46:02, 26.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 4. sad\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  12%|â–ˆâ–        | 87/730 [40:42<5:38:21, 31.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 40. I\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  15%|â–ˆâ–        | 108/730 [51:20<5:18:34, 30.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 41. you\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  15%|â–ˆâ–        | 109/730 [51:44<4:59:29, 28.94s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0016.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  18%|â–ˆâ–Š        | 129/730 [1:02:03<5:33:48, 33.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 42. he\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  20%|â–ˆâ–ˆ        | 149/730 [1:13:16<5:40:56, 35.21s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9965.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  21%|â–ˆâ–ˆ        | 150/730 [1:13:42<5:16:14, 32.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 43. she\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  23%|â–ˆâ–ˆâ–       | 171/730 [1:26:20<5:31:02, 35.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 44. it\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  26%|â–ˆâ–ˆâ–‹       | 192/730 [1:37:12<4:25:16, 29.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 45. we\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  29%|â–ˆâ–ˆâ–‰       | 213/730 [1:49:34<4:06:13, 28.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 46. you (plural)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  30%|â–ˆâ–ˆâ–ˆ       | 219/730 [1:53:24<4:53:29, 34.46s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0032.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  31%|â–ˆâ–ˆâ–ˆ       | 223/730 [1:55:40<4:53:05, 34.69s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9905.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  32%|â–ˆâ–ˆâ–ˆâ–      | 234/730 [2:01:47<5:02:29, 36.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 47. they\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  35%|â–ˆâ–ˆâ–ˆâ–      | 252/730 [2:12:37<5:48:32, 43.75s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9979.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  35%|â–ˆâ–ˆâ–ˆâ–      | 255/730 [2:13:59<4:19:59, 32.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 48. Hello\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 260/730 [2:16:29<3:48:30, 29.17s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0038.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 276/730 [2:24:50<3:38:48, 28.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 49. How are you\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 297/730 [2:39:04<4:36:54, 38.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: Extra\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing videos:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 298/730 [2:39:42<4:36:55, 38.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 5. Beautiful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 306/730 [2:44:26<4:18:54, 36.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 50. Alright\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 310/730 [2:47:05<4:38:15, 39.75s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0043.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 327/730 [2:56:27<3:21:02, 29.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 51. Good Morning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 337/730 [3:02:27<3:35:36, 32.92s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9932.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 339/730 [3:03:27<3:24:07, 31.32s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9934.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 348/730 [3:08:34<3:36:40, 34.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 52. Good afternoon\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 355/730 [3:13:27<4:30:56, 43.35s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0051.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 370/730 [3:21:35<3:15:03, 32.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 53. Good evening\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 390/730 [3:35:56<3:46:13, 39.92s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9999.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 391/730 [3:36:26<3:28:57, 36.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 54. Good night\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0002.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 394/730 [3:38:13<3:28:31, 37.24s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0055.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 406/730 [3:46:08<3:14:13, 35.97s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9946.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 412/730 [3:49:58<3:26:29, 38.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 55. Thank you\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing videos:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 413/730 [3:50:29<3:12:57, 36.52s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0007.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 416/730 [3:52:14<3:08:37, 36.04s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0059.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 417/730 [3:52:57<3:17:56, 37.94s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0060.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 431/730 [3:59:47<1:28:03, 17.67s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9986.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 433/730 [4:00:08<1:08:42, 13.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 56. Pleased\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0009.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 438/730 [4:02:40<2:12:26, 27.21s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_0063.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 454/730 [4:12:54<2:51:06, 37.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 6. Ugly\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 462/730 [4:17:54<2:30:09, 33.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 7. Deaf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 467/730 [4:20:45<2:25:23, 33.17s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9849.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 470/730 [4:22:12<2:10:05, 30.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 78. long\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 484/730 [4:29:02<1:42:58, 25.12s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9300.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 488/730 [4:30:42<1:40:33, 24.93s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9458.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 491/730 [4:32:01<1:43:24, 25.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: extra\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing videos:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 492/730 [4:32:32<1:48:45, 27.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 79. short\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 496/730 [4:34:34<1:55:46, 29.69s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5190.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 512/730 [4:41:40<1:43:43, 28.55s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9463.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 513/730 [4:42:09<1:43:13, 28.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 8. Blind\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 518/730 [4:44:36<1:43:29, 29.29s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9853.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 519/730 [4:45:11<1:49:31, 31.15s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9854.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 521/730 [4:46:11<1:45:49, 30.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 80. tall\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 542/730 [4:55:32<1:23:01, 26.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 81. wide\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 563/730 [5:05:48<1:23:36, 30.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 87. hot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing videos:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 564/730 [5:06:22<1:26:46, 31.37s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5137.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 569/730 [5:08:43<1:20:10, 29.88s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5295.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 570/730 [5:09:10<1:17:21, 29.01s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5296.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 580/730 [5:13:35<1:11:04, 28.43s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9410.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 584/730 [5:15:42<1:17:17, 31.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 88. cold\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5139.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 586/730 [5:16:48<1:18:24, 32.67s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_5219.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 600/730 [5:22:59<58:41, 27.09s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9413.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 604/730 [5:25:16<1:09:22, 33.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 89. warm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 625/730 [5:34:21<25:28, 14.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 90. cool\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 645/730 [5:43:37<37:07, 26.21s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9499.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 646/730 [5:44:05<37:21, 26.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 91. new\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 664/730 [5:51:52<28:10, 25.61s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9500.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 667/730 [5:53:12<27:10, 25.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 97. dry\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 678/730 [5:59:03<25:29, 29.42s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9279.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 688/730 [6:03:58<21:40, 30.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 98. sick\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 697/730 [6:08:36<15:06, 27.47s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9280.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 705/730 [6:12:13<11:25, 27.40s/it]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file temp_MVI_9444.MOV, 6220800 bytes wanted but 0 bytes read,at frame 56/57, at time 2.24/2.24 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "Processing videos:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 709/730 [6:14:20<10:52, 31.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing word category: 99. healthy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 730/730 [6:24:54<00:00, 31.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Done. Extracted 45295 frames and 45295 keypoints sets.\n",
            "Processed 41 word categories: ['1. loud', '2. quiet', '3. happy', '36. light', '39. famous', '4. sad', '40. I', '41. you', '42. he', '43. she', '44. it', '45. we', '46. you (plural)', '47. they', '48. Hello', '49. How are you', '5. Beautiful', '50. Alright', '51. Good Morning', '52. Good afternoon', '53. Good evening', '54. Good night', '55. Thank you', '56. Pleased', '6. Ugly', '7. Deaf', '78. long', '79. short', '8. Blind', '80. tall', '81. wide', '87. hot', '88. cold', '89. warm', '90. cool', '91. new', '97. dry', '98. sick', '99. healthy', 'Extra', 'extra']\n",
            "\n",
            "Sample batch shapes:\n",
            "Images: (8, 16, 368, 368, 3)\n",
            "Heatmaps: (8, 16, 46, 46, 33)\n",
            "\n",
            "Processing complete! Data saved to /content/drive/MyDrive/processed_openpose_data\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "# Cell 8: Main Processing Pipeline\n",
        "def main():\n",
        "    # 1. Process videos to frames + keypoints\n",
        "    print(\"Processing videos to OpenPose data...\")\n",
        "    frame_paths, keypoints = process_videos_to_openpose(\n",
        "        DATASET_PATH,\n",
        "        os.path.join(OUTPUT_PATH, \"frames\")\n",
        "    )\n",
        "\n",
        "    # Check if frame_paths is empty\n",
        "    if not frame_paths:\n",
        "        print(\"Error: No frames extracted. Check your dataset path and video format.\")\n",
        "        return  # Exit the function if no frames are found\n",
        "\n",
        "    # 2. Split data (video-wise)\n",
        "    video_names = list(set([os.path.basename(os.path.dirname(f)) for f in frame_paths]))\n",
        "    train_vids, test_vids = train_test_split(video_names, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data = [(f, k) for f, k in zip(frame_paths, keypoints)\n",
        "                 if os.path.basename(os.path.dirname(f)) in train_vids]\n",
        "    test_data = [(f, k) for f, k in zip(frame_paths, keypoints)\n",
        "                if os.path.basename(os.path.dirname(f)) in test_vids]\n",
        "\n",
        "    # 3. Create generators\n",
        "    train_gen = OpenPoseSequenceGenerator(\n",
        "        [d[0] for d in train_data],\n",
        "        [d[1] for d in train_data],\n",
        "        augment=True\n",
        "    )\n",
        "    test_gen = OpenPoseSequenceGenerator(\n",
        "        [d[0] for d in test_data],\n",
        "        [d[1] for d in test_data],\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    # 4. Verify output\n",
        "    sample_images, sample_heatmaps = train_gen[0]\n",
        "    print(\"\\nSample batch shapes:\")\n",
        "    print(f\"Images: {sample_images.shape}\")  # (batch, seq_len, 368, 368, 3)\n",
        "    print(f\"Heatmaps: {sample_heatmaps.shape}\")  # (batch, seq_len, 46, 46, 25)\n",
        "\n",
        "    # 5. Save metadata\n",
        "    np.savez(os.path.join(OUTPUT_PATH, \"dataset_meta.npz\"),\n",
        "             train_frames=[d[0] for d in train_data],\n",
        "             test_frames=[d[0] for d in test_data],\n",
        "             input_shape=sample_images.shape[1:],\n",
        "             output_shape=sample_heatmaps.shape[1:])\n",
        "\n",
        "    print(f\"\\nProcessing complete! Data saved to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KEsjJ-LYYcy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOaG31Hv5ltz"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "PROCESSED_PATH = Path(\"/content/drive/MyDrive/processed_openpose_data\")\n",
        "OUTPUT_ROOT = Path(\"/content/drive/MyDrive/openpose_train_test_data\")\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "\n",
        "# Augmentation configuration\n",
        "TRAIN_AUGMENTOR = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    fill_mode='constant'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m5ML3LLU08v",
        "outputId": "eb0add5b-a4c1-4276-fb57-a529b5c598dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 45295 images in /content/drive/MyDrive/processed_openpose_data\n",
            "Train: 36236 | Test: 9059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36236/36236 [4:13:01<00:00,  2.39it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9059/9059 [08:32<00:00, 17.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processing complete.\n",
            "Train files: 103952\n",
            "Test files: 9059\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "PROCESSED_PATH = Path(\"/content/drive/MyDrive/processed_openpose_data\")  # Use local path, not Google Drive\n",
        "OUTPUT_ROOT = Path(\"/content/drive/MyDrive/openpose_train_test_data\")\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "\n",
        "# Dummy augmentation (replace with your real augmenter)\n",
        "class DummyAugmentor:\n",
        "    def random_transform(self, img):\n",
        "        return cv2.flip(img, 1)  # Simple horizontal flip\n",
        "\n",
        "TRAIN_AUGMENTOR = DummyAugmentor()\n",
        "\n",
        "# ========== MAIN ==========\n",
        "def process_and_split():\n",
        "    all_samples = []\n",
        "    for root, _, files in os.walk(PROCESSED_PATH):\n",
        "        if not files:\n",
        "            continue\n",
        "        category = Path(root).name\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.png')):\n",
        "                all_samples.append((Path(root) / file, category))\n",
        "\n",
        "    print(f\"Found {len(all_samples)} images in {PROCESSED_PATH}\")\n",
        "\n",
        "    file_paths = [str(p[0]) for p in all_samples]\n",
        "    categories = [p[1] for p in all_samples]\n",
        "\n",
        "    train_files, test_files = train_test_split(\n",
        "        file_paths,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=categories\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(train_files)} | Test: {len(test_files)}\")\n",
        "\n",
        "    train_dir = OUTPUT_ROOT / \"train\"\n",
        "    test_dir = OUTPUT_ROOT / \"test\"\n",
        "    train_dir.mkdir(parents=True, exist_ok=True)\n",
        "    test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def process_one(src_path, target_dir, augment):\n",
        "        src = Path(src_path)\n",
        "        category = src.parent.name\n",
        "        dest_dir = target_dir / category\n",
        "        dest_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Copy original image\n",
        "        dest_file = dest_dir / src.name\n",
        "        if not dest_file.exists():\n",
        "            shutil.copy2(src, dest_file)\n",
        "\n",
        "        # Augment\n",
        "        if augment:\n",
        "            img = cv2.imread(str(src))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            for i in range(1):  # 1 augmented image per original\n",
        "                aug_img = TRAIN_AUGMENTOR.random_transform(img)\n",
        "                aug_file = dest_dir / f\"aug_{i}_{src.name}\"\n",
        "                cv2.imwrite(str(aug_file), cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    def process_split_parallel(file_list, target_dir, augment=False):\n",
        "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "            list(tqdm(executor.map(lambda f: process_one(f, target_dir, augment), file_list), total=len(file_list)))\n",
        "\n",
        "    # Process both splits\n",
        "    process_split_parallel(train_files, train_dir, augment=True)\n",
        "    process_split_parallel(test_files, test_dir, augment=False)\n",
        "\n",
        "    print(\"âœ… Processing complete.\")\n",
        "    print(f\"Train files: {sum(len(f) for _, _, f in os.walk(train_dir))}\")\n",
        "    print(f\"Test files: {sum(len(f) for _, _, f in os.walk(test_dir))}\")\n",
        "\n",
        "# Run the processing\n",
        "if __name__ == \"__main__\":\n",
        "    process_and_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti6toRijl0AG",
        "outputId": "90e26d99-8b8c-4d26-9588-0d4bd2ba988c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in the .npz file: ['train_frames', 'test_frames', 'input_shape', 'output_shape']\n",
            "\n",
            "Key: train_frames\n",
            "['/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_0.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_1.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_2.jpg'\n",
            " ...\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_59.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_60.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_61.jpg']\n",
            "\n",
            "Key: test_frames\n",
            "['/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_0.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_1.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_2.jpg'\n",
            " ...\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_63.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_64.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_65.jpg']\n",
            "\n",
            "Key: input_shape\n",
            "[ 16 368 368   3]\n",
            "\n",
            "Key: output_shape\n",
            "[16 46 46 33]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the .npz file\n",
        "data = np.load('/content/drive/MyDrive/processed_openpose_data/dataset_meta.npz', allow_pickle=True)\n",
        "\n",
        "# List all keys in the file\n",
        "print(\"Keys in the .npz file:\", data.files)\n",
        "\n",
        "# Example: Inspect the content of each key\n",
        "for key in data.files:\n",
        "    print(f\"\\nKey: {key}\")\n",
        "    print(data[key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq32jMM4l6lk",
        "outputId": "0655d720-9ec2-4efd-ff32-9691c3a7f9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in the .npz file: ['train_frames', 'test_frames', 'input_shape', 'output_shape']\n",
            "\n",
            "Key: train_frames\n",
            "['/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_0.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_1.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_2.jpg'\n",
            " ...\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_59.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_60.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/99. healthy/99. healthy_temp_MVI_9532_61.jpg']\n",
            "\n",
            "Key: test_frames\n",
            "['/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_0.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_1.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/48. Hello/48. Hello_temp_MVI_0029_2.jpg'\n",
            " ...\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_63.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_64.jpg'\n",
            " '/content/drive/MyDrive/processed_openpose_data/frames/97. dry/97. dry_temp_MVI_9523_65.jpg']\n",
            "\n",
            "Key: input_shape\n",
            "[ 16 368 368   3]\n",
            "\n",
            "Key: output_shape\n",
            "[16 46 46 33]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path where keypoints are saved\n",
        "keypoint_file = \"/content/drive/MyDrive/processed_openpose_data/dataset_meta.npz\"\n",
        "\n",
        "# Load the .npz file\n",
        "data = np.load(keypoint_file, allow_pickle=True)\n",
        "\n",
        "# List all keys in the file\n",
        "print(\"Keys in the .npz file:\", data.files)\n",
        "\n",
        "# Example: Inspect the content of each key\n",
        "for key in data.files:\n",
        "    print(f\"\\nKey: {key}\")\n",
        "    print(data[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8CFsHMlaqxT",
        "outputId": "4ba35919-b412-4a48-cf6c-98f532d0160f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key found: train_frames\n",
            "Key found: test_frames\n",
            "Key found: input_shape\n",
            "Key found: output_shape\n"
          ]
        }
      ],
      "source": [
        "for key in data.files:\n",
        "    print(\"Key found:\", key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fwq2PdHa1Qh",
        "outputId": "5f4aa1e0-77c2-4fba-8d3b-fcbc8de95d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (35819,)\n",
            "Test shape: (9476,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Train shape:\", data['train_frames'].shape)\n",
        "print(\"Test shape:\", data['test_frames'].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LhjFbs8bEx2",
        "outputId": "c59981bf-dde3-45fc-cbfa-98508ca43eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of train_frames: <class 'numpy.ndarray'>\n",
            "First item: /content/drive/MyDrive/processed_openpose_data/frames/1. loud/1. loud_temp_MVI_5177_0.jpg\n"
          ]
        }
      ],
      "source": [
        "print(\"Type of train_frames:\", type(data['train_frames']))\n",
        "print(\"First item:\", data['train_frames'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MFgMb-ebREA",
        "outputId": "cf73fc65-fde9-45a5-b751-03560da360fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keypoint file not found at: /content/drive/MyDrive/processed_openpose_data/keypoints/1. loud/1. loud_temp_MVI_5177_0.npy\n"
          ]
        }
      ],
      "source": [
        "# Guess a path for a corresponding keypoint file\n",
        "frame_path = data['train_frames'][0]\n",
        "keypoint_path = frame_path.replace(\"frames\", \"keypoints\").replace(\".jpg\", \".npy\")\n",
        "\n",
        "# Load it (if it exists)\n",
        "import os\n",
        "\n",
        "if os.path.exists(keypoint_path):\n",
        "    keypoints = np.load(keypoint_path)\n",
        "    print(\"Loaded keypoints shape:\", keypoints.shape)\n",
        "else:\n",
        "    print(\"Keypoint file not found at:\", keypoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cfP920pbkks",
        "outputId": "0ae0c01a-192f-41cd-bca9-716513edb1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 .npy files.\n",
            "[]\n",
            "Found 0 .json files.\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Adjust this base path if needed\n",
        "base_dir = \"/content/drive/MyDrive/processed_openpose_data\"\n",
        "\n",
        "# Find all .npy files under the directory tree\n",
        "npy_files = glob.glob(os.path.join(base_dir, \"**\", \"*.npy\"), recursive=True)\n",
        "print(f\"Found {len(npy_files)} .npy files.\")\n",
        "print(npy_files[:10])  # show the first few\n",
        "\n",
        "# If youâ€™re using OpenPose JSON output:\n",
        "json_files = glob.glob(os.path.join(base_dir, \"**\", \"*.json\"), recursive=True)\n",
        "print(f\"Found {len(json_files)} .json files.\")\n",
        "print(json_files[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz1DWy99cwk1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "# Initialize MediaPipe Holistic once\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "def extract_keypoints(frame_path):\n",
        "    # Read & preprocess\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if frame is None:\n",
        "        print(f\"Could not load image at {frame_path}\")\n",
        "        return None\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process with the same Holistic instance\n",
        "    with mp_holistic.Holistic(static_image_mode=True) as holistic:\n",
        "        results = holistic.process(image_rgb)\n",
        "\n",
        "    kp = []\n",
        "\n",
        "    # Pose: 33 landmarks Ã— [x, y, z, visibility]\n",
        "    if results.pose_landmarks:\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            kp.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
        "    else:\n",
        "        kp.extend([0.0] * 33 * 4)\n",
        "\n",
        "    # Face: 468 landmarks Ã— [x, y, z]\n",
        "    if results.face_landmarks:\n",
        "        for lm in results.face_landmarks.landmark:\n",
        "            kp.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        kp.extend([0.0] * 468 * 3)\n",
        "\n",
        "    # Left hand: 21 landmarks Ã— [x, y, z]\n",
        "    if results.left_hand_landmarks:\n",
        "        for lm in results.left_hand_landmarks.landmark:\n",
        "            kp.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        kp.extend([0.0] * 21 * 3)\n",
        "\n",
        "    # Right hand: 21 landmarks Ã— [x, y, z]\n",
        "    if results.right_hand_landmarks:\n",
        "        for lm in results.right_hand_landmarks.landmark:\n",
        "            kp.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        kp.extend([0.0] * 21 * 3)\n",
        "\n",
        "    return np.array(kp, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZJyTe929hqDq",
        "outputId": "1bb2dec7-ee63-4a0c-b3be-7e9d9379a5ae"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-85415ed02349>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoint_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_frame_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoint_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_keypoints.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-85415ed02349>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoint_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_frame_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_keypoints\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoint_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_keypoints.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-19f550ddaafb>\u001b[0m in \u001b[0;36mextract_keypoints\u001b[0;34m(frame_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Read & preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not load image at {frame_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load metadata\n",
        "meta = np.load(\"/content/drive/MyDrive/processed_openpose_data/dataset_meta.npz\", allow_pickle=True)\n",
        "train_frame_paths = meta['train_frames']\n",
        "test_frame_paths  = meta['test_frames']\n",
        "\n",
        "# Extract and save\n",
        "os.makedirs(keypoint_save_dir, exist_ok=True)\n",
        "\n",
        "train_keypoints = [extract_keypoints(p) for p in train_frame_paths]\n",
        "train_keypoints = np.stack([kp for kp in train_keypoints if kp is not None])\n",
        "np.save(os.path.join(keypoint_save_dir, 'train_keypoints.npy'), train_keypoints)\n",
        "\n",
        "test_keypoints = [extract_keypoints(p) for p in test_frame_paths]\n",
        "test_keypoints = np.stack([kp for kp in test_keypoints if kp is not None])\n",
        "np.save(os.path.join(keypoint_save_dir, 'test_keypoints.npy'), test_keypoints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "iNLs0MYtZ4MM",
        "outputId": "08d1d620-8fbe-4f2f-9470-2964f323f6b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (35819,)\n",
            "Sample shape: ()\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'reshape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-22eae7a6d576>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Pose: 33*4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# Face: 468*3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m468\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m468\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'reshape'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the .npz file\n",
        "data = np.load(\"/content/drive/MyDrive/processed_openpose_data/dataset_meta.npz\", allow_pickle=True)\n",
        "\n",
        "train_frames = data['train_frames']\n",
        "print(\"Train shape:\", train_frames.shape)\n",
        "\n",
        "# Choose a sample (1 frame)\n",
        "sample = train_frames[0]\n",
        "\n",
        "# Total length will tell us what kind of data it includes\n",
        "print(\"Sample shape:\", sample.shape)\n",
        "\n",
        "# Try visualizing by assuming standard OpenPose-like layout:\n",
        "# Let's guess it contains (pose: 25*3), or (pose + hands + face)\n",
        "# We'll try splitting it manually if needed\n",
        "\n",
        "# If sample is 1662 long, it's probably:\n",
        "#  - 33 pose landmarks * 4 (x, y, z, visibility)\n",
        "#  - 468 face landmarks * 3\n",
        "#  - 21 hand landmarks * 3 each (left and right)\n",
        "\n",
        "# Pose: 33*4\n",
        "pose = sample[0:33*4].reshape(33, 4)\n",
        "# Face: 468*3\n",
        "face = sample[33*4:33*4 + 468*3].reshape(468, 3)\n",
        "# Left hand: 21*3\n",
        "lh_start = 33*4 + 468*3\n",
        "lh = sample[lh_start:lh_start + 21*3].reshape(21, 3)\n",
        "# Right hand: 21*3\n",
        "rh = sample[lh_start + 21*3:].reshape(21, 3)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(\"Sample Keypoints from train_frames\")\n",
        "\n",
        "def plot_points(points, color, label):\n",
        "    xs = points[:, 0]\n",
        "    ys = -points[:, 1]\n",
        "    plt.scatter(xs, ys, label=label, color=color, s=10)\n",
        "\n",
        "plot_points(pose, 'blue', 'Pose')\n",
        "plot_points(face, 'green', 'Face')\n",
        "plot_points(lh, 'red', 'Left Hand')\n",
        "plot_points(rh, 'orange', 'Right Hand')\n",
        "\n",
        "plt.legend()\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egGzwQhEi-7O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configuration\n",
        "DATASET_PATH = \"/content/drive/MyDrive/datauk.zip\"  # Update this\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/new_processed_data\"\n",
        "IMG_SIZE = (368, 368)  # OpenPose standard input size\n",
        "HEATMAP_SIZE = (46, 46)  # 1/8 of input size\n",
        "SEQ_LENGTH = 16  # For temporal processing\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvq-te0oaefT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "from moviepy.editor import VideoFileClip\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False,\n",
        "                   min_detection_confidence=0.5,\n",
        "                   min_tracking_confidence=0.5)\n",
        "\n",
        "def process_videos_to_openpose(video_dir, output_dir, frames_per_video=50):\n",
        "    \"\"\"Extracts frames and OpenPose keypoints from videos, handling nested zip structures\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    all_frames = []\n",
        "    all_keypoints = []\n",
        "    processed_words = set()\n",
        "\n",
        "    print(f\"ğŸ“‚ Scanning: {video_dir}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    def process_video_file(video_path, word_category=\"\"):\n",
        "        \"\"\"Helper function to process individual video files\"\"\"\n",
        "        try:\n",
        "            clip = VideoFileClip(video_path)\n",
        "            video_name = Path(video_path).stem\n",
        "\n",
        "            # Create output subdirectory for word category\n",
        "            word_output_dir = os.path.join(output_dir, word_category)\n",
        "            os.makedirs(word_output_dir, exist_ok=True)\n",
        "\n",
        "            frames = []\n",
        "            keypoints = []\n",
        "\n",
        "            for frame_idx, frame in enumerate(clip.iter_frames()):\n",
        "                if frame_idx % max(1, int(clip.fps / frames_per_video)) == 0:\n",
        "                    # Save frame\n",
        "                    frame_path = os.path.join(word_output_dir,\n",
        "                                            f\"{word_category}_{video_name}_{frame_idx}.jpg\")\n",
        "                    success = cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    if not success:\n",
        "                        print(f\"âŒ Failed to save: {frame_path}\")\n",
        "                        continue\n",
        "\n",
        "                    frames.append(frame_path)\n",
        "\n",
        "                    # Check the frame type\n",
        "                    print(f\"Processing frame {frame_idx}: Shape={frame.shape}, Type={frame.dtype}\")\n",
        "\n",
        "                    # Get keypoints\n",
        "                    results = pose.process(frame)\n",
        "                    if results.pose_landmarks:\n",
        "                        kps = []\n",
        "                        for landmark in results.pose_landmarks.landmark:\n",
        "                            kps.extend([landmark.x, landmark.y, landmark.visibility])\n",
        "                        keypoints.append(kps)\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ No keypoints found for frame {frame_idx}\")\n",
        "                        keypoints.append([])\n",
        "\n",
        "            clip.close()\n",
        "            return frames, keypoints\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error processing {video_path}: {str(e)}\")\n",
        "            return [], []\n",
        "\n",
        "    if video_dir.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(video_dir, 'r') as zip_ref:\n",
        "            # Find all video files, preserving directory structure\n",
        "            video_files = [f for f in zip_ref.namelist()\n",
        "                         if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "            print(f\"ğŸ Found {len(video_files)} videos in zip\")\n",
        "\n",
        "            for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
        "                # Extract word category from path (assuming structure: words/word1/videos.mp4)\n",
        "                path_parts = Path(video_file).parts\n",
        "                word_category = path_parts[-2] if len(path_parts) > 1 else \"unknown\"\n",
        "\n",
        "                if word_category not in processed_words:\n",
        "                    print(f\"\\nProcessing word category: {word_category}\")\n",
        "                    processed_words.add(word_category)\n",
        "\n",
        "                # Extract and process video\n",
        "                with zip_ref.open(video_file) as video_data:\n",
        "                    temp_path = f\"temp_{Path(video_file).name}\"\n",
        "                    with open(temp_path, \"wb\") as temp_file:\n",
        "                        temp_file.write(video_data.read())\n",
        "\n",
        "                    frames, kps = process_video_file(temp_path, word_category)\n",
        "                    all_frames.extend(frames)\n",
        "                    all_keypoints.extend(kps)\n",
        "\n",
        "                    # Clean up\n",
        "                    if os.path.exists(temp_path):\n",
        "                        os.remove(temp_path)\n",
        "    else:\n",
        "        # Process directory with word subfolders\n",
        "        for root, dirs, files in os.walk(video_dir):\n",
        "            video_files = [f for f in files if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "            if video_files:\n",
        "                word_category = Path(root).name\n",
        "                print(f\"\\nProcessing word category: {word_category}\")\n",
        "\n",
        "                for video_file in tqdm(video_files, desc=f\"Processing {word_category}\"):\n",
        "                    video_path = os.path.join(root, video_file)\n",
        "                    frames, kps = process_video_file(video_path, word_category)\n",
        "                    all_frames.extend(frames)\n",
        "                    all_keypoints.extend(kps)\n",
        "\n",
        "    print(f\"\\nâœ… Done. Extracted {len(all_frames)} frames and {len(all_keypoints)} keypoints sets.\")\n",
        "    print(f\"Processed {len(processed_words)} word categories: {sorted(processed_words)}\")\n",
        "\n",
        "    return all_frames, all_keypoints\n",
        "\n",
        "# Example usage:\n",
        "# frames, keypoints = process_videos_to_openpose(\"dataset.zip\", \"output_frames\")\n",
        "# OR\n",
        "# frames, keypoints = process_videos_to_openpose(\"dataset_folder\", \"output_frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbyucba7jltG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Cell 6: Data Augmentation\n",
        "def augment_data(image, heatmap):\n",
        "    \"\"\"Applies random augmentations\"\"\"\n",
        "    # Random flip\n",
        "    if np.random.rand() > 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "        heatmap = np.flip(heatmap, axis=1)\n",
        "\n",
        "    # Random brightness\n",
        "    alpha = np.random.uniform(0.8, 1.2)\n",
        "    image = cv2.convertScaleAbs(image, alpha=alpha)\n",
        "\n",
        "    return image, heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q08N9DhkhAH",
        "outputId": "c700c95c-75e1-491f-8555-6a1b4ce65d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy==1.23.5 opencv-python tqdm mediapipe tensorflow==2.12.0 moviepy gdown\n",
        "\n",
        "# Optional: install unzip utility (only needed if you're downloading zip files)\n",
        "!apt-get -qq install -y unzip\n",
        "\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfGSHXE9kXPL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZAvvTfAj110h",
        "outputId": "3a50a247-bdb2-413a-fa8f-8aa2e9379d2a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a1473e9eb81c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# 1. Extract ZIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mextract_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZIP_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXTRACT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# 2. Gather video files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a1473e9eb81c>\u001b[0m in \u001b[0;36mextract_zip\u001b[0;34m(zip_file, extract_to)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Extracted ZIP to: {extract_to}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1756\u001b[0;31m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1757\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import mediapipe as mp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ZIP_FILE_PATH  = \"/content/drive/MyDrive/datauk.zip\"\n",
        "EXTRACT_PATH   = \"/content/drive/MyDrive/new_processed_data\"\n",
        "FRAMES_PER_VID = 50\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "POSE_LM, FACE_LM, HAND_LM = 33, 468, 21\n",
        "KP_LEN = POSE_LM * 4 + FACE_LM * 3 + HAND_LM * 3 * 2  # 1662\n",
        "VIDEO_EXTS = {'.mp4', '.avi', '.mov', '.mkv', '.flv'}\n",
        "\n",
        "# â”€â”€â”€ Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"âœ… Extracted ZIP to: {extract_to}\")\n",
        "\n",
        "\n",
        "def extract_kps_from_frame(rgb_frame):\n",
        "    holo = mp.solutions.holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "    res = holo.process(rgb_frame)\n",
        "    kp = np.zeros(KP_LEN, dtype=np.float32)\n",
        "    idx = 0\n",
        "\n",
        "    # Pose\n",
        "    if res.pose_landmarks:\n",
        "        for lm in res.pose_landmarks.landmark:\n",
        "            kp[idx:idx+4] = (lm.x, lm.y, lm.z, lm.visibility)\n",
        "            idx += 4\n",
        "    else:\n",
        "        idx += POSE_LM * 4\n",
        "\n",
        "    # Face\n",
        "    if res.face_landmarks:\n",
        "        for lm in res.face_landmarks.landmark:\n",
        "            kp[idx:idx+3] = (lm.x, lm.y, lm.z)\n",
        "            idx += 3\n",
        "    else:\n",
        "        idx += FACE_LM * 3\n",
        "\n",
        "    # Hands\n",
        "    for hand in (res.left_hand_landmarks, res.right_hand_landmarks):\n",
        "        if hand:\n",
        "            for lm in hand.landmark:\n",
        "                kp[idx:idx+3] = (lm.x, lm.y, lm.z)\n",
        "                idx += 3\n",
        "        else:\n",
        "            idx += HAND_LM * 3\n",
        "\n",
        "    return kp\n",
        "\n",
        "\n",
        "def process_video(video_path, output_dir):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total <= 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "\n",
        "    indices = np.linspace(0, total - 1, FRAMES_PER_VID, dtype=int)\n",
        "    out_dir = Path(output_dir) / video_path.parent.name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    results = []\n",
        "    for idx in indices:\n",
        "        frame_file = out_dir / f\"{video_path.stem}_{idx}.jpg\"\n",
        "        kp_file = out_dir / f\"{video_path.stem}_{idx}_kps.npy\"\n",
        "\n",
        "        if frame_file.exists() and kp_file.exists():\n",
        "            results.append((str(frame_file), str(kp_file)))\n",
        "            continue\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        kp = extract_kps_from_frame(rgb)\n",
        "\n",
        "        cv2.imwrite(str(frame_file), frame)\n",
        "        np.save(str(kp_file), kp)\n",
        "        results.append((str(frame_file), str(kp_file)))\n",
        "\n",
        "    cap.release()\n",
        "    return results\n",
        "\n",
        "# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Extract ZIP\n",
        "    extract_zip(ZIP_FILE_PATH, EXTRACT_PATH)\n",
        "\n",
        "    # 2. Gather video files\n",
        "    video_files = [p for p in Path(EXTRACT_PATH).rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
        "    print(f\"ğŸ¥ Found {len(video_files)} video files.\")\n",
        "\n",
        "    if not video_files:\n",
        "        print(\"Error: No video files found. Check your ZIP content and path.\")\n",
        "        exit(1)\n",
        "\n",
        "    # 3. Process videos sequentially\n",
        "    all_data = []\n",
        "    for video_path in video_files:\n",
        "        all_data.extend(process_video(video_path, EXTRACT_PATH))\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"Error: No frames or keypoints extracted.\")\n",
        "        exit(1)\n",
        "\n",
        "    frame_paths, kp_paths = zip(*all_data)\n",
        "\n",
        "    # 4. Train/test split by video folder\n",
        "    video_names = [Path(fp).parent.name for fp in frame_paths]\n",
        "    unique_vids = list(set(video_names))\n",
        "    train_vids, test_vids = train_test_split(unique_vids, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_frames = [fp for fp, kp in zip(frame_paths, kp_paths) if Path(fp).parent.name in train_vids]\n",
        "    test_frames  = [fp for fp, kp in zip(frame_paths, kp_paths) if Path(fp).parent.name in test_vids]\n",
        "\n",
        "    train_kp =     [kp for fp, kp in zip(frame_paths, kp_paths) if Path(fp).parent.name in train_vids]\n",
        "    test_kp =      [kp for fp, kp in zip(frame_paths, kp_paths) if Path(fp).parent.name in test_vids]\n",
        "\n",
        "    # 5. Save metadata\n",
        "    np.savez(Path(EXTRACT_PATH) / \"dataset_meta.npz\",\n",
        "             train_frames=train_frames,\n",
        "             test_frames=test_frames,\n",
        "             train_kp=train_kp,\n",
        "             test_kp=test_kp,\n",
        "             input_shape=(FRAMES_PER_VID, 368, 368, 3),\n",
        "             output_shape=(FRAMES_PER_VID, KP_LEN))\n",
        "\n",
        "    print(f\"âœ… Extraction complete. Data and metadata saved in: {EXTRACT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Ys14Ls8CjtLo",
        "outputId": "9bcb6084-5f25-4163-c2cb-a4dbc9898875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Extracted ZIP file to /content/drive/MyDrive/new_processed_data\n",
            "Found 730 video files.\n"
          ]
        },
        {
          "ename": "BrokenProcessPool",
          "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2633a7ec1ad9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_single_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvid_paths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfut\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mall_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mall_keypoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import mediapipe as mp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ZIP_FILE_PATH  = \"/content/drive/MyDrive/datauk.zip\"\n",
        "EXTRACT_PATH   = \"/content/drive/MyDrive/new_processed_data\"\n",
        "FRAMES_PER_VID = 50\n",
        "NUM_WORKERS    = 4   # Adjust to your available CPU cores\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Precompute total keypoint length\n",
        "POSE_LM, FACE_LM, HAND_LM = 33, 468, 21\n",
        "KP_LEN = POSE_LM*4 + FACE_LM*3 + HAND_LM*3*2  # 1662\n",
        "\n",
        "# MediaPipe model factory (each process will call this)\n",
        "def make_holistic():\n",
        "    mp_h = mp.solutions.holistic\n",
        "    return mp_h.Holistic(static_image_mode=False,\n",
        "                         min_detection_confidence=0.5,\n",
        "                         min_tracking_confidence=0.5)\n",
        "\n",
        "# Extract keypoints from a single frame (RGB ndarray)\n",
        "def extract_kps_from_frame(holo, rgb_frame):\n",
        "    res = holo.process(rgb_frame)\n",
        "    kp = np.zeros(KP_LEN, dtype=np.float32)\n",
        "    idx = 0\n",
        "\n",
        "    # Pose (x,y,z,vis)\n",
        "    if res.pose_landmarks:\n",
        "        for lm in res.pose_landmarks.landmark:\n",
        "            kp[idx:idx+4] = (lm.x, lm.y, lm.z, lm.visibility)\n",
        "            idx += 4\n",
        "    else:\n",
        "        idx += POSE_LM*4\n",
        "\n",
        "    # Face (x,y,z)\n",
        "    if res.face_landmarks:\n",
        "        for lm in res.face_landmarks.landmark:\n",
        "            kp[idx:idx+3] = (lm.x, lm.y, lm.z)\n",
        "            idx += 3\n",
        "    else:\n",
        "        idx += FACE_LM*3\n",
        "\n",
        "    # Left & Right Hand (x,y,z each)\n",
        "    for hand in (res.left_hand_landmarks, res.right_hand_landmarks):\n",
        "        if hand:\n",
        "            for lm in hand.landmark:\n",
        "                kp[idx:idx+3] = (lm.x, lm.y, lm.z)\n",
        "                idx += 3\n",
        "        else:\n",
        "            idx += HAND_LM*3\n",
        "\n",
        "    return kp\n",
        "\n",
        "# Process one video file: extract frames + keypoints\n",
        "def process_single_video(video_path, frames_per_video=FRAMES_PER_VID):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total <= 0:\n",
        "        cap.release()\n",
        "        return [], []\n",
        "\n",
        "    # Determine frame indices to sample\n",
        "    indices = np.linspace(0, total - 1, frames_per_video, dtype=int)\n",
        "    frames, kps = [], []\n",
        "    holo = make_holistic()\n",
        "\n",
        "    for i in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame_bgr = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "\n",
        "        # Convert and extract\n",
        "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        kp = extract_kps_from_frame(holo, rgb)\n",
        "\n",
        "        # Save frame image\n",
        "        out_dir = Path(EXTRACT_PATH) / video_path.parent.name\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_img = out_dir / f\"{video_path.stem}_{i}.jpg\"\n",
        "        cv2.imwrite(str(out_img), frame_bgr)\n",
        "\n",
        "        frames.append(str(out_img))\n",
        "        kps.append(kp)\n",
        "\n",
        "    cap.release()\n",
        "    return frames, kps\n",
        "\n",
        "# Extract zip file\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        print(f\"âœ… Extracted ZIP file to {extract_to}\")\n",
        "\n",
        "# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Extract videos from zip file\n",
        "    extract_zip(ZIP_FILE_PATH, EXTRACT_PATH)\n",
        "\n",
        "    # 2. Gather all video paths (now that files are extracted)\n",
        "    vid_paths = list(Path(EXTRACT_PATH).rglob(\"*.*\"))\n",
        "    vid_paths = [p for p in vid_paths if p.suffix.lower() in (\".mp4\", \".avi\", \".mov\", \".mkv\", \".flv\")]\n",
        "    print(f\"Found {len(vid_paths)} video files.\")\n",
        "\n",
        "    if not vid_paths:\n",
        "        print(\"No valid video files found in the extracted directory.\")\n",
        "        exit()\n",
        "\n",
        "    # 3. Parallel processing of videos to frames + keypoints\n",
        "    all_frames, all_keypoints = [], []\n",
        "    with ProcessPoolExecutor(NUM_WORKERS) as exe:\n",
        "        futures = {exe.submit(process_single_video, vp): vp for vp in vid_paths}\n",
        "        for fut in as_completed(futures):\n",
        "            frames, kps = fut.result()\n",
        "            all_frames.extend(frames)\n",
        "            all_keypoints.extend(kps)\n",
        "\n",
        "    print(f\"Extracted {len(all_frames)} frames and {len(all_keypoints)} keypoint sets.\")\n",
        "\n",
        "    # 4. Train/test split by video directory name\n",
        "    vids_by_cat = {}\n",
        "    for fp, kp in zip(all_frames, all_keypoints):\n",
        "        cat = Path(fp).parent.name\n",
        "        vids_by_cat.setdefault(cat, []).append((fp, kp))\n",
        "\n",
        "    cats = list(vids_by_cat)\n",
        "    train_cats, test_cats = train_test_split(cats, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data = [item for c in train_cats for item in vids_by_cat[c]]\n",
        "    test_data  = [item for c in test_cats  for item in vids_by_cat[c]]\n",
        "\n",
        "    # 5. Save .npy files\n",
        "    np.save(Path(EXTRACT_PATH) / \"train_keypoints.npy\", np.stack([kp for _, kp in train_data]))\n",
        "    np.save(Path(EXTRACT_PATH) / \"test_keypoints.npy\",  np.stack([kp for _, kp in test_data ]))\n",
        "    np.savez(Path(EXTRACT_PATH) / \"dataset_meta.npz\",\n",
        "             train_frames=[fp for fp, _ in train_data],\n",
        "             test_frames =[fp for fp, _ in test_data],\n",
        "             input_shape  = (FRAMES_PER_VID, 368, 368, 3),  # Example shape (adjust as needed)\n",
        "             output_shape = (FRAMES_PER_VID, KP_LEN))\n",
        "\n",
        "    print(\"âœ”ï¸ Processing complete. Files saved under:\", EXTRACT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGsU4qygkLO0",
        "outputId": "13261f4d-762b-4942-ab05-2bbad902a47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 45295 images in /content/drive/MyDrive/processed_openpose_data\n",
            "Train: 36236 | Test: 9059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36236/36236 [39:40<00:00, 15.22it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9059/9059 [07:45<00:00, 19.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processing complete.\n",
            "Train files: 72472\n",
            "Test files: 9059\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "PROCESSED_PATH = Path(\"/content/drive/MyDrive/processed_openpose_data\")  # Use local path, not Google Drive\n",
        "OUTPUT_ROOT = Path(\"/content/drive/MyDrive/newopenpose_train_test_data\")\n",
        "TEST_SIZE = 0.2\n",
        "SEED = 42\n",
        "\n",
        "# Dummy augmentation (replace with your real augmenter)\n",
        "class DummyAugmentor:\n",
        "    def random_transform(self, img):\n",
        "        return cv2.flip(img, 1)  # Simple horizontal flip\n",
        "\n",
        "TRAIN_AUGMENTOR = DummyAugmentor()\n",
        "\n",
        "# ========== MAIN ==========\n",
        "def process_and_split():\n",
        "    all_samples = []\n",
        "    for root, _, files in os.walk(PROCESSED_PATH):\n",
        "        if not files:\n",
        "            continue\n",
        "        category = Path(root).name\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.png')):\n",
        "                all_samples.append((Path(root) / file, category))\n",
        "\n",
        "    print(f\"Found {len(all_samples)} images in {PROCESSED_PATH}\")\n",
        "\n",
        "    file_paths = [str(p[0]) for p in all_samples]\n",
        "    categories = [p[1] for p in all_samples]\n",
        "\n",
        "    train_files, test_files = train_test_split(\n",
        "        file_paths,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=categories\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(train_files)} | Test: {len(test_files)}\")\n",
        "\n",
        "    train_dir = OUTPUT_ROOT / \"train\"\n",
        "    test_dir = OUTPUT_ROOT / \"test\"\n",
        "    train_dir.mkdir(parents=True, exist_ok=True)\n",
        "    test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def process_one(src_path, target_dir, augment):\n",
        "        src = Path(src_path)\n",
        "        category = src.parent.name\n",
        "        dest_dir = target_dir / category\n",
        "        dest_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Copy original image\n",
        "        dest_file = dest_dir / src.name\n",
        "        if not dest_file.exists():\n",
        "            shutil.copy2(src, dest_file)\n",
        "\n",
        "        # Augment\n",
        "        if augment:\n",
        "            img = cv2.imread(str(src))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            for i in range(1):  # 1 augmented image per original\n",
        "                aug_img = TRAIN_AUGMENTOR.random_transform(img)\n",
        "                aug_file = dest_dir / f\"aug_{i}_{src.name}\"\n",
        "                cv2.imwrite(str(aug_file), cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    def process_split_parallel(file_list, target_dir, augment=False):\n",
        "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "            list(tqdm(executor.map(lambda f: process_one(f, target_dir, augment), file_list), total=len(file_list)))\n",
        "\n",
        "    # Process both splits\n",
        "    process_split_parallel(train_files, train_dir, augment=True)\n",
        "    process_split_parallel(test_files, test_dir, augment=False)\n",
        "\n",
        "    print(\"âœ… Processing complete.\")\n",
        "    print(f\"Train files: {sum(len(f) for _, _, f in os.walk(train_dir))}\")\n",
        "    print(f\"Test files: {sum(len(f) for _, _, f in os.walk(test_dir))}\")\n",
        "\n",
        "# Run the processing\n",
        "if __name__ == \"__main__\":\n",
        "    process_and_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "khZd9phyIpls",
        "outputId": "9d0f8cb1-81a2-47de-f6a3-8114254175d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Scanning for duplicates in /content/drive/MyDrive/newopenpose_train_test_data/train\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_image_hash(image_path):\n",
        "    with open(image_path, 'rb') as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "def remove_duplicate_images(folder: Path):\n",
        "    print(f\"ğŸ” Scanning for duplicates in {folder}\")\n",
        "    seen_hashes = set()\n",
        "    deleted = 0\n",
        "\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.endswith((\".jpg\", \".png\")):\n",
        "                file_path = Path(root) / file\n",
        "                try:\n",
        "                    img_hash = compute_image_hash(file_path)\n",
        "                    if img_hash in seen_hashes:\n",
        "                        os.remove(file_path)\n",
        "                        deleted += 1\n",
        "                    else:\n",
        "                        seen_hashes.add(img_hash)\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Error reading {file_path}: {e}\")\n",
        "\n",
        "    print(f\"âœ… Done. Removed {deleted} duplicate frames from {folder}.\")\n",
        "\n",
        "# Run this only after splitting is done\n",
        "if __name__ == \"__main__\":\n",
        "    train_folder = Path(\"/content/drive/MyDrive/newopenpose_train_test_data/train\")\n",
        "    test_folder  = Path(\"/content/drive/MyDrive/newopenpose_train_test_data/test\")\n",
        "\n",
        "    remove_duplicate_images(train_folder)\n",
        "    remove_duplicate_images(test_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6tCmOw65wa9",
        "outputId": "7ce7d3da-b822-4746-d143-418f73ddc280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train frames: 36219 | Test frames: 9076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36219/36219 [53:36<00:00, 11.26it/s]\n",
            "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9076/9076 [12:58<00:00, 11.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Motion filtering and copying complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "PROCESSED_PATH = Path(\"/content/drive/MyDrive/processed_openpose_data\")  # Existing frames directory\n",
        "OUTPUT_ROOT    = Path(\"/content/drive/MyDrive/feature_train_test_data\")  # Destination root\n",
        "MOTION_THRESHOLD = 500  # adjust sensitivity\n",
        "MAX_WORKERS = 8  # threads for parallel processing\n",
        "\n",
        "# Dummy augmentor (replace with your own)\n",
        "class DummyAugmentor:\n",
        "    def random_transform(self, img):\n",
        "        return cv2.flip(img, 1)\n",
        "\n",
        "TRAIN_AUGMENTOR = DummyAugmentor()\n",
        "\n",
        "# ========== FUNCTIONS ==========\n",
        "\n",
        "def has_motion(prev_frame: np.ndarray, curr_frame: np.ndarray, threshold: float = MOTION_THRESHOLD) -> bool:\n",
        "    \"\"\"\n",
        "    Compare two frames and return True if motion exceeds threshold.\n",
        "    \"\"\"\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    diff = cv2.absdiff(prev_gray, curr_gray)\n",
        "    return np.sum(diff) > threshold\n",
        "\n",
        "\n",
        "def save_if_motion(prev_frame: np.ndarray, frame_path: Path, dest_dir: Path, augment: bool = False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Copy frame to dest_dir if motion detected, optionally augment.\n",
        "    Returns current frame if saved (for next comparison), else returns prev_frame.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(str(frame_path))\n",
        "    if img is None:\n",
        "        return prev_frame\n",
        "\n",
        "    if prev_frame is None or has_motion(prev_frame, img):\n",
        "        # ensure category directory\n",
        "        cat_dir = dest_dir / frame_path.parent.name\n",
        "        cat_dir.mkdir(parents=True, exist_ok=True)\n",
        "        # copy original frame\n",
        "        dst = cat_dir / frame_path.name\n",
        "        shutil.copy2(frame_path, dst)\n",
        "\n",
        "        # augment if requested\n",
        "        if augment:\n",
        "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            aug = TRAIN_AUGMENTOR.random_transform(rgb)\n",
        "            aug_bgr = cv2.cvtColor(aug, cv2.COLOR_RGB2BGR)\n",
        "            aug_name = f\"aug_{frame_path.name}\"\n",
        "            cv2.imwrite(str(cat_dir / aug_name), aug_bgr)\n",
        "\n",
        "        return img\n",
        "\n",
        "    # no motion: skip saving\n",
        "    return prev_frame\n",
        "\n",
        "\n",
        "def process_files_with_motion(file_list: list, dest_root: Path, augment: bool = False):\n",
        "    \"\"\"\n",
        "    Process a list of frame file paths:\n",
        "    - Filters out static frames via motion detection.\n",
        "    - Copies frames with motion to dest_root.\n",
        "    - Optionally augments images when augment=True.\n",
        "    \"\"\"\n",
        "    prev_by_cat = {}\n",
        "    # ensure root exists\n",
        "    dest_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _process(path_str):\n",
        "        frame_path = Path(path_str)\n",
        "        cat = frame_path.parent.name\n",
        "        prev = prev_by_cat.get(cat)\n",
        "        new_prev = save_if_motion(prev, frame_path, dest_root, augment)\n",
        "        prev_by_cat[cat] = new_prev\n",
        "\n",
        "    # Use ThreadPoolExecutor for speed\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        list(tqdm(executor.map(_process, file_list), total=len(file_list), desc=(\"Augment\" if augment else \"Filter\")))\n",
        "# if __name__ == '__main__':\n",
        "#     # Example: assume you have two lists of frame paths already\n",
        "#     train_frame_list = [str(p) for p in (PROCESSED_PATH / 'category1').glob('*.jpg')]\n",
        "#     test_frame_list  = [str(p) for p in (PROCESSED_PATH / 'category2').glob('*.jpg')]\n",
        "\n",
        "#     # Process train (with augmentation)\n",
        "#     process_files_with_motion(train_frame_list, OUTPUT_ROOT / 'train', augment=True)\n",
        "#     # Process test (without augmentation)\n",
        "#     process_files_with_motion(test_frame_list, OUTPUT_ROOT / 'test', augment=False)\n",
        "\n",
        "#     print(\"âœ… Motion filtering and copying complete.\")\n",
        "if __name__ == '__main__':\n",
        "    frames_root = PROCESSED_PATH / 'frames'\n",
        "\n",
        "    # Get all frame paths grouped by category\n",
        "    train_frame_list = []\n",
        "    test_frame_list = []\n",
        "\n",
        "    for category_dir in frames_root.iterdir():\n",
        "        if category_dir.is_dir():\n",
        "            all_images = sorted(category_dir.glob(\"*.jpg\"))\n",
        "            split_idx = int(len(all_images) * 0.8)\n",
        "            train_frame_list.extend([str(p) for p in all_images[:split_idx]])\n",
        "            test_frame_list.extend([str(p) for p in all_images[split_idx:]])\n",
        "\n",
        "    print(f\"Train frames: {len(train_frame_list)} | Test frames: {len(test_frame_list)}\")\n",
        "\n",
        "    # Process train (with augmentation)\n",
        "    process_files_with_motion(train_frame_list, OUTPUT_ROOT / 'train', augment=True)\n",
        "\n",
        "    # Process test (without augmentation)\n",
        "    process_files_with_motion(test_frame_list, OUTPUT_ROOT / 'test', augment=False)\n",
        "\n",
        "    print(\"âœ… Motion filtering and copying complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAhSTPkmFEH7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl+wgHC1eAW/2NtRb19aY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}